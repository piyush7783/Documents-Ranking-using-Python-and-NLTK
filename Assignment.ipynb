{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Piyush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Piyush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "### Importing all mandatory Libraries\n",
    "\n",
    "import re\n",
    "import glob\n",
    "import math\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from  more_itertools import unique_everseen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Reading the names of all text files in the current directory\n",
    "\n",
    "lst = glob.glob('./*.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.\\\\doc1.txt',\n",
       " '.\\\\doc10.txt',\n",
       " '.\\\\doc11.txt',\n",
       " '.\\\\doc12.txt',\n",
       " '.\\\\doc13.txt',\n",
       " '.\\\\doc14.txt',\n",
       " '.\\\\doc2.txt',\n",
       " '.\\\\doc3.txt',\n",
       " '.\\\\doc4.txt',\n",
       " '.\\\\doc5.txt',\n",
       " '.\\\\doc6.txt',\n",
       " '.\\\\doc7.txt',\n",
       " '.\\\\doc8.txt',\n",
       " '.\\\\doc9.txt']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Removing unwanted symbols\n",
    "\n",
    "lst = [i.replace(\".\\\\\",\"\") for i in lst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['doc1.txt',\n",
       " 'doc10.txt',\n",
       " 'doc11.txt',\n",
       " 'doc12.txt',\n",
       " 'doc13.txt',\n",
       " 'doc14.txt',\n",
       " 'doc2.txt',\n",
       " 'doc3.txt',\n",
       " 'doc4.txt',\n",
       " 'doc5.txt',\n",
       " 'doc6.txt',\n",
       " 'doc7.txt',\n",
       " 'doc8.txt',\n",
       " 'doc9.txt']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Reading Contents of above Text files\n",
    "\n",
    "final =[]\n",
    "for i in range(len(lst)):\n",
    "    files = open(lst[i],\"r\")\n",
    "    var = \" \".join(files.readlines())\n",
    "    var = var.replace(\".\",\"\")\n",
    "    var = var.replace(\",\",\"\")\n",
    "    var1 = int(re.search(r'\\d+', lst[i]).group())\n",
    "    final.append([var,var1])\n",
    "    files.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents =>  1\n",
      "The field of information retrieval also covers supporting users in browsing or filtering document\n",
      " collections or further processing a set of retrieved documents Given a set of documents\n",
      " clustering is the task of coming up with a good grouping of the documents based on their\n",
      " contents\n",
      "------------------------------------------------\n",
      "Documents =>  10\n",
      "Ahead of bypolls to eight Assembly seats in Gujarat on November 3 the BJP and the\n",
      " Congress are allegedly using tactics to make a dent in their respective rival camps with a\n",
      " wave of defections being reported in Kaprada and Dangs Assembly seats of south Gujarat\n",
      "------------------------------------------------\n",
      "Documents =>  11\n",
      "The party then decided to field a candidate against whom we had been fighting for two\n",
      " decades There is no future in the BJP so we have joined the Congress There are many\n",
      " more BJP workers and leaders from Kaprada seats who will join Congress once the\n",
      " names of candidates from both parties are officially announced\n",
      "------------------------------------------------\n",
      "Documents =>  12\n",
      "The risks of getting COVID-19 are higher in crowded and inadequately ventilated spaces\n",
      " where infected people spend long periods of time together in close proximity These\n",
      " environments are where the virus appears to spreads by respiratory droplets or aerosols\n",
      " more efficiently so taking precautions is even more important\n",
      "------------------------------------------------\n",
      "Documents =>  13\n",
      "The most common symptoms of COVID-19 are fever dry cough and tiredness Other\n",
      " symptoms that are less common and may affect some patients include loss of taste or\n",
      " smell aches and pains headache sore throat nasal congestion red eyes diarrhoea or a\n",
      " skin rash\n",
      "------------------------------------------------\n",
      "Documents =>  14\n",
      "If COVID-19 is spreading in your community stay safe by taking some simple precautions\n",
      " such as physical distancing wearing a mask keeping rooms well ventilated avoiding\n",
      " crowds cleaning your hands and coughing into a bent elbow or tissue Check local advice\n",
      " where you live and work\n",
      "------------------------------------------------\n",
      "Documents =>  2\n",
      "A document is relevant if it addresses the stated information need not because it just happens\n",
      " to contain all the words in the query This distinction is often misunderstood in practice because\n",
      " the information needed is not overt But nevertheless an information need is present If a user\n",
      " types python into a web search engine they might be wanting to know where they can purchase\n",
      " a pet python\n",
      "------------------------------------------------\n",
      "Documents =>  3\n",
      "Since 2008 most of top stars in the cricketing world have been taking a collective break\n",
      " from international and domestic cricket to take part in the Indian Premier League (IPL) that\n",
      " happens mostly in a two-month window between March and May In short during this time\n",
      " period the IPL has been the cynosure of the cricketing world’s eyes at large\n",
      "------------------------------------------------\n",
      "Documents =>  4\n",
      "It was in this setting that the T20 format captured the public imagination Why it seems\n",
      " really hard to believe that India were not too keen on fielding a team for the inaugural\n",
      " World T20 and had to be subtly arm-twisted into doing so by former ICC president Ehsan\n",
      " Mani\n",
      "------------------------------------------------\n",
      "Documents =>  5\n",
      "This also explains the love-hate relationship between the English cricketing establishment\n",
      " and the IPL — the English players have either usually had to pick IPL over their domestic\n",
      " commitments or have been passed over entirely No doubt the IPL has the first-mover\n",
      " advantage\n",
      "------------------------------------------------\n",
      "Documents =>  6\n",
      "The Indian movie industry is rolling again with several filmmakers shooting for a slew of big\n",
      " releases next year after being forced to shutter up by the virus outbreakThe first major\n",
      " production house to bell the cat has been Excel Entertainment\n",
      "------------------------------------------------\n",
      "Documents =>  7\n",
      "Other measures introduced to help prevent the spread of the virus have included\n",
      " appointing health-care experts or designated COVID officers to monitor every shooting\n",
      " schedule and ensure that the strictest standards of hygiene and sanitization\n",
      "------------------------------------------------\n",
      "Documents =>  8\n",
      "The Indian entertainment industry is estimated to have lost at least 1000 crore rupees\n",
      " ($133 million) since the virus lockdown was imposed in March Producers said the\n",
      " precautions were necessary to resume work especially for those who had no other means\n",
      " to a livelihood or had been rendered jobless by the pandemic\n",
      "------------------------------------------------\n",
      "Documents =>  9\n",
      "A war of words broke out between BJP and Congress after Rahul Gandhi tweeted suggesting\n",
      " that Prime Minister Narendra Modi was off the mark over his comment about possible use of\n",
      " wind turbines to draw moisture out of the atmosphere with ministers saying it was the Congress\n",
      " leader who was uninformed\n",
      "------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "### Printing Contents of above Text files\n",
    "\n",
    "for i in final:\n",
    "    print(\"Documents => \",i[1])\n",
    "    print(i[0])\n",
    "    print(\"------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removing_stop_words(lst):\n",
    "    lst1 = []\n",
    "    for i in lst:\n",
    "        if i.lower() not in stop_words:\n",
    "            lst1.append(i.lower())\n",
    "    return(lst1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Removing Stop words\n",
    "\n",
    "token_without_stopwords_docs_wise =[]\n",
    "token_without_stopwords =[]\n",
    "for i in range(len(final)):\n",
    "    tokenized_word = nltk.word_tokenize(final[i][0])\n",
    "    temp_var=[]\n",
    "    for j in tokenized_word:\n",
    "        if j.lower() not in stop_words:\n",
    "            token_without_stopwords.append([j.lower(),final[i][1]])\n",
    "            temp_var.append(j.lower())\n",
    "    token_without_stopwords_docs_wise.append([temp_var,final[i][1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['field', 1],\n",
       " ['information', 1],\n",
       " ['retrieval', 1],\n",
       " ['also', 1],\n",
       " ['covers', 1],\n",
       " ['supporting', 1],\n",
       " ['users', 1],\n",
       " ['browsing', 1],\n",
       " ['filtering', 1],\n",
       " ['document', 1],\n",
       " ['collections', 1],\n",
       " ['processing', 1],\n",
       " ['set', 1],\n",
       " ['retrieved', 1],\n",
       " ['documents', 1],\n",
       " ['given', 1],\n",
       " ['set', 1],\n",
       " ['documents', 1],\n",
       " ['clustering', 1],\n",
       " ['task', 1],\n",
       " ['coming', 1],\n",
       " ['good', 1],\n",
       " ['grouping', 1],\n",
       " ['documents', 1],\n",
       " ['based', 1],\n",
       " ['contents', 1],\n",
       " ['ahead', 10],\n",
       " ['bypolls', 10],\n",
       " ['eight', 10],\n",
       " ['assembly', 10],\n",
       " ['seats', 10],\n",
       " ['gujarat', 10],\n",
       " ['november', 10],\n",
       " ['3', 10],\n",
       " ['bjp', 10],\n",
       " ['congress', 10],\n",
       " ['allegedly', 10],\n",
       " ['using', 10],\n",
       " ['tactics', 10],\n",
       " ['make', 10],\n",
       " ['dent', 10],\n",
       " ['respective', 10],\n",
       " ['rival', 10],\n",
       " ['camps', 10],\n",
       " ['wave', 10],\n",
       " ['defections', 10],\n",
       " ['reported', 10],\n",
       " ['kaprada', 10],\n",
       " ['dangs', 10],\n",
       " ['assembly', 10],\n",
       " ['seats', 10],\n",
       " ['south', 10],\n",
       " ['gujarat', 10],\n",
       " ['party', 11],\n",
       " ['decided', 11],\n",
       " ['field', 11],\n",
       " ['candidate', 11],\n",
       " ['fighting', 11],\n",
       " ['two', 11],\n",
       " ['decades', 11],\n",
       " ['future', 11],\n",
       " ['bjp', 11],\n",
       " ['joined', 11],\n",
       " ['congress', 11],\n",
       " ['many', 11],\n",
       " ['bjp', 11],\n",
       " ['workers', 11],\n",
       " ['leaders', 11],\n",
       " ['kaprada', 11],\n",
       " ['seats', 11],\n",
       " ['join', 11],\n",
       " ['congress', 11],\n",
       " ['names', 11],\n",
       " ['candidates', 11],\n",
       " ['parties', 11],\n",
       " ['officially', 11],\n",
       " ['announced', 11],\n",
       " ['risks', 12],\n",
       " ['getting', 12],\n",
       " ['covid-19', 12],\n",
       " ['higher', 12],\n",
       " ['crowded', 12],\n",
       " ['inadequately', 12],\n",
       " ['ventilated', 12],\n",
       " ['spaces', 12],\n",
       " ['infected', 12],\n",
       " ['people', 12],\n",
       " ['spend', 12],\n",
       " ['long', 12],\n",
       " ['periods', 12],\n",
       " ['time', 12],\n",
       " ['together', 12],\n",
       " ['close', 12],\n",
       " ['proximity', 12],\n",
       " ['environments', 12],\n",
       " ['virus', 12],\n",
       " ['appears', 12],\n",
       " ['spreads', 12],\n",
       " ['respiratory', 12],\n",
       " ['droplets', 12],\n",
       " ['aerosols', 12],\n",
       " ['efficiently', 12],\n",
       " ['taking', 12],\n",
       " ['precautions', 12],\n",
       " ['even', 12],\n",
       " ['important', 12],\n",
       " ['common', 13],\n",
       " ['symptoms', 13],\n",
       " ['covid-19', 13],\n",
       " ['fever', 13],\n",
       " ['dry', 13],\n",
       " ['cough', 13],\n",
       " ['tiredness', 13],\n",
       " ['symptoms', 13],\n",
       " ['less', 13],\n",
       " ['common', 13],\n",
       " ['may', 13],\n",
       " ['affect', 13],\n",
       " ['patients', 13],\n",
       " ['include', 13],\n",
       " ['loss', 13],\n",
       " ['taste', 13],\n",
       " ['smell', 13],\n",
       " ['aches', 13],\n",
       " ['pains', 13],\n",
       " ['headache', 13],\n",
       " ['sore', 13],\n",
       " ['throat', 13],\n",
       " ['nasal', 13],\n",
       " ['congestion', 13],\n",
       " ['red', 13],\n",
       " ['eyes', 13],\n",
       " ['diarrhoea', 13],\n",
       " ['skin', 13],\n",
       " ['rash', 13],\n",
       " ['covid-19', 14],\n",
       " ['spreading', 14],\n",
       " ['community', 14],\n",
       " ['stay', 14],\n",
       " ['safe', 14],\n",
       " ['taking', 14],\n",
       " ['simple', 14],\n",
       " ['precautions', 14],\n",
       " ['physical', 14],\n",
       " ['distancing', 14],\n",
       " ['wearing', 14],\n",
       " ['mask', 14],\n",
       " ['keeping', 14],\n",
       " ['rooms', 14],\n",
       " ['well', 14],\n",
       " ['ventilated', 14],\n",
       " ['avoiding', 14],\n",
       " ['crowds', 14],\n",
       " ['cleaning', 14],\n",
       " ['hands', 14],\n",
       " ['coughing', 14],\n",
       " ['bent', 14],\n",
       " ['elbow', 14],\n",
       " ['tissue', 14],\n",
       " ['check', 14],\n",
       " ['local', 14],\n",
       " ['advice', 14],\n",
       " ['live', 14],\n",
       " ['work', 14],\n",
       " ['document', 2],\n",
       " ['relevant', 2],\n",
       " ['addresses', 2],\n",
       " ['stated', 2],\n",
       " ['information', 2],\n",
       " ['need', 2],\n",
       " ['happens', 2],\n",
       " ['contain', 2],\n",
       " ['words', 2],\n",
       " ['query', 2],\n",
       " ['distinction', 2],\n",
       " ['often', 2],\n",
       " ['misunderstood', 2],\n",
       " ['practice', 2],\n",
       " ['information', 2],\n",
       " ['needed', 2],\n",
       " ['overt', 2],\n",
       " ['nevertheless', 2],\n",
       " ['information', 2],\n",
       " ['need', 2],\n",
       " ['present', 2],\n",
       " ['user', 2],\n",
       " ['types', 2],\n",
       " ['python', 2],\n",
       " ['web', 2],\n",
       " ['search', 2],\n",
       " ['engine', 2],\n",
       " ['might', 2],\n",
       " ['wanting', 2],\n",
       " ['know', 2],\n",
       " ['purchase', 2],\n",
       " ['pet', 2],\n",
       " ['python', 2],\n",
       " ['since', 3],\n",
       " ['2008', 3],\n",
       " ['top', 3],\n",
       " ['stars', 3],\n",
       " ['cricketing', 3],\n",
       " ['world', 3],\n",
       " ['taking', 3],\n",
       " ['collective', 3],\n",
       " ['break', 3],\n",
       " ['international', 3],\n",
       " ['domestic', 3],\n",
       " ['cricket', 3],\n",
       " ['take', 3],\n",
       " ['part', 3],\n",
       " ['indian', 3],\n",
       " ['premier', 3],\n",
       " ['league', 3],\n",
       " ['(', 3],\n",
       " ['ipl', 3],\n",
       " [')', 3],\n",
       " ['happens', 3],\n",
       " ['mostly', 3],\n",
       " ['two-month', 3],\n",
       " ['window', 3],\n",
       " ['march', 3],\n",
       " ['may', 3],\n",
       " ['short', 3],\n",
       " ['time', 3],\n",
       " ['period', 3],\n",
       " ['ipl', 3],\n",
       " ['cynosure', 3],\n",
       " ['cricketing', 3],\n",
       " ['world', 3],\n",
       " ['’', 3],\n",
       " ['eyes', 3],\n",
       " ['large', 3],\n",
       " ['setting', 4],\n",
       " ['t20', 4],\n",
       " ['format', 4],\n",
       " ['captured', 4],\n",
       " ['public', 4],\n",
       " ['imagination', 4],\n",
       " ['seems', 4],\n",
       " ['really', 4],\n",
       " ['hard', 4],\n",
       " ['believe', 4],\n",
       " ['india', 4],\n",
       " ['keen', 4],\n",
       " ['fielding', 4],\n",
       " ['team', 4],\n",
       " ['inaugural', 4],\n",
       " ['world', 4],\n",
       " ['t20', 4],\n",
       " ['subtly', 4],\n",
       " ['arm-twisted', 4],\n",
       " ['former', 4],\n",
       " ['icc', 4],\n",
       " ['president', 4],\n",
       " ['ehsan', 4],\n",
       " ['mani', 4],\n",
       " ['also', 5],\n",
       " ['explains', 5],\n",
       " ['love-hate', 5],\n",
       " ['relationship', 5],\n",
       " ['english', 5],\n",
       " ['cricketing', 5],\n",
       " ['establishment', 5],\n",
       " ['ipl', 5],\n",
       " ['—', 5],\n",
       " ['english', 5],\n",
       " ['players', 5],\n",
       " ['either', 5],\n",
       " ['usually', 5],\n",
       " ['pick', 5],\n",
       " ['ipl', 5],\n",
       " ['domestic', 5],\n",
       " ['commitments', 5],\n",
       " ['passed', 5],\n",
       " ['entirely', 5],\n",
       " ['doubt', 5],\n",
       " ['ipl', 5],\n",
       " ['first-mover', 5],\n",
       " ['advantage', 5],\n",
       " ['indian', 6],\n",
       " ['movie', 6],\n",
       " ['industry', 6],\n",
       " ['rolling', 6],\n",
       " ['several', 6],\n",
       " ['filmmakers', 6],\n",
       " ['shooting', 6],\n",
       " ['slew', 6],\n",
       " ['big', 6],\n",
       " ['releases', 6],\n",
       " ['next', 6],\n",
       " ['year', 6],\n",
       " ['forced', 6],\n",
       " ['shutter', 6],\n",
       " ['virus', 6],\n",
       " ['outbreakthe', 6],\n",
       " ['first', 6],\n",
       " ['major', 6],\n",
       " ['production', 6],\n",
       " ['house', 6],\n",
       " ['bell', 6],\n",
       " ['cat', 6],\n",
       " ['excel', 6],\n",
       " ['entertainment', 6],\n",
       " ['measures', 7],\n",
       " ['introduced', 7],\n",
       " ['help', 7],\n",
       " ['prevent', 7],\n",
       " ['spread', 7],\n",
       " ['virus', 7],\n",
       " ['included', 7],\n",
       " ['appointing', 7],\n",
       " ['health-care', 7],\n",
       " ['experts', 7],\n",
       " ['designated', 7],\n",
       " ['covid', 7],\n",
       " ['officers', 7],\n",
       " ['monitor', 7],\n",
       " ['every', 7],\n",
       " ['shooting', 7],\n",
       " ['schedule', 7],\n",
       " ['ensure', 7],\n",
       " ['strictest', 7],\n",
       " ['standards', 7],\n",
       " ['hygiene', 7],\n",
       " ['sanitization', 7],\n",
       " ['indian', 8],\n",
       " ['entertainment', 8],\n",
       " ['industry', 8],\n",
       " ['estimated', 8],\n",
       " ['lost', 8],\n",
       " ['least', 8],\n",
       " ['1000', 8],\n",
       " ['crore', 8],\n",
       " ['rupees', 8],\n",
       " ['(', 8],\n",
       " ['$', 8],\n",
       " ['133', 8],\n",
       " ['million', 8],\n",
       " [')', 8],\n",
       " ['since', 8],\n",
       " ['virus', 8],\n",
       " ['lockdown', 8],\n",
       " ['imposed', 8],\n",
       " ['march', 8],\n",
       " ['producers', 8],\n",
       " ['said', 8],\n",
       " ['precautions', 8],\n",
       " ['necessary', 8],\n",
       " ['resume', 8],\n",
       " ['work', 8],\n",
       " ['especially', 8],\n",
       " ['means', 8],\n",
       " ['livelihood', 8],\n",
       " ['rendered', 8],\n",
       " ['jobless', 8],\n",
       " ['pandemic', 8],\n",
       " ['war', 9],\n",
       " ['words', 9],\n",
       " ['broke', 9],\n",
       " ['bjp', 9],\n",
       " ['congress', 9],\n",
       " ['rahul', 9],\n",
       " ['gandhi', 9],\n",
       " ['tweeted', 9],\n",
       " ['suggesting', 9],\n",
       " ['prime', 9],\n",
       " ['minister', 9],\n",
       " ['narendra', 9],\n",
       " ['modi', 9],\n",
       " ['mark', 9],\n",
       " ['comment', 9],\n",
       " ['possible', 9],\n",
       " ['use', 9],\n",
       " ['wind', 9],\n",
       " ['turbines', 9],\n",
       " ['draw', 9],\n",
       " ['moisture', 9],\n",
       " ['atmosphere', 9],\n",
       " ['ministers', 9],\n",
       " ['saying', 9],\n",
       " ['congress', 9],\n",
       " ['leader', 9],\n",
       " ['uninformed', 9]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_without_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Sorting tokenized words\n",
    "\n",
    "token_without_stopwords.sort(key=lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['$', 8],\n",
       " ['(', 3],\n",
       " ['(', 8],\n",
       " [')', 3],\n",
       " [')', 8],\n",
       " ['1000', 8],\n",
       " ['133', 8],\n",
       " ['2008', 3],\n",
       " ['3', 10],\n",
       " ['aches', 13],\n",
       " ['addresses', 2],\n",
       " ['advantage', 5],\n",
       " ['advice', 14],\n",
       " ['aerosols', 12],\n",
       " ['affect', 13],\n",
       " ['ahead', 10],\n",
       " ['allegedly', 10],\n",
       " ['also', 1],\n",
       " ['also', 5],\n",
       " ['announced', 11],\n",
       " ['appears', 12],\n",
       " ['appointing', 7],\n",
       " ['arm-twisted', 4],\n",
       " ['assembly', 10],\n",
       " ['assembly', 10],\n",
       " ['atmosphere', 9],\n",
       " ['avoiding', 14],\n",
       " ['based', 1],\n",
       " ['believe', 4],\n",
       " ['bell', 6],\n",
       " ['bent', 14],\n",
       " ['big', 6],\n",
       " ['bjp', 10],\n",
       " ['bjp', 11],\n",
       " ['bjp', 11],\n",
       " ['bjp', 9],\n",
       " ['break', 3],\n",
       " ['broke', 9],\n",
       " ['browsing', 1],\n",
       " ['bypolls', 10],\n",
       " ['camps', 10],\n",
       " ['candidate', 11],\n",
       " ['candidates', 11],\n",
       " ['captured', 4],\n",
       " ['cat', 6],\n",
       " ['check', 14],\n",
       " ['cleaning', 14],\n",
       " ['close', 12],\n",
       " ['clustering', 1],\n",
       " ['collections', 1],\n",
       " ['collective', 3],\n",
       " ['coming', 1],\n",
       " ['comment', 9],\n",
       " ['commitments', 5],\n",
       " ['common', 13],\n",
       " ['common', 13],\n",
       " ['community', 14],\n",
       " ['congestion', 13],\n",
       " ['congress', 10],\n",
       " ['congress', 11],\n",
       " ['congress', 11],\n",
       " ['congress', 9],\n",
       " ['congress', 9],\n",
       " ['contain', 2],\n",
       " ['contents', 1],\n",
       " ['cough', 13],\n",
       " ['coughing', 14],\n",
       " ['covers', 1],\n",
       " ['covid', 7],\n",
       " ['covid-19', 12],\n",
       " ['covid-19', 13],\n",
       " ['covid-19', 14],\n",
       " ['cricket', 3],\n",
       " ['cricketing', 3],\n",
       " ['cricketing', 3],\n",
       " ['cricketing', 5],\n",
       " ['crore', 8],\n",
       " ['crowded', 12],\n",
       " ['crowds', 14],\n",
       " ['cynosure', 3],\n",
       " ['dangs', 10],\n",
       " ['decades', 11],\n",
       " ['decided', 11],\n",
       " ['defections', 10],\n",
       " ['dent', 10],\n",
       " ['designated', 7],\n",
       " ['diarrhoea', 13],\n",
       " ['distancing', 14],\n",
       " ['distinction', 2],\n",
       " ['document', 1],\n",
       " ['document', 2],\n",
       " ['documents', 1],\n",
       " ['documents', 1],\n",
       " ['documents', 1],\n",
       " ['domestic', 3],\n",
       " ['domestic', 5],\n",
       " ['doubt', 5],\n",
       " ['draw', 9],\n",
       " ['droplets', 12],\n",
       " ['dry', 13],\n",
       " ['efficiently', 12],\n",
       " ['ehsan', 4],\n",
       " ['eight', 10],\n",
       " ['either', 5],\n",
       " ['elbow', 14],\n",
       " ['engine', 2],\n",
       " ['english', 5],\n",
       " ['english', 5],\n",
       " ['ensure', 7],\n",
       " ['entertainment', 6],\n",
       " ['entertainment', 8],\n",
       " ['entirely', 5],\n",
       " ['environments', 12],\n",
       " ['especially', 8],\n",
       " ['establishment', 5],\n",
       " ['estimated', 8],\n",
       " ['even', 12],\n",
       " ['every', 7],\n",
       " ['excel', 6],\n",
       " ['experts', 7],\n",
       " ['explains', 5],\n",
       " ['eyes', 13],\n",
       " ['eyes', 3],\n",
       " ['fever', 13],\n",
       " ['field', 1],\n",
       " ['field', 11],\n",
       " ['fielding', 4],\n",
       " ['fighting', 11],\n",
       " ['filmmakers', 6],\n",
       " ['filtering', 1],\n",
       " ['first', 6],\n",
       " ['first-mover', 5],\n",
       " ['forced', 6],\n",
       " ['format', 4],\n",
       " ['former', 4],\n",
       " ['future', 11],\n",
       " ['gandhi', 9],\n",
       " ['getting', 12],\n",
       " ['given', 1],\n",
       " ['good', 1],\n",
       " ['grouping', 1],\n",
       " ['gujarat', 10],\n",
       " ['gujarat', 10],\n",
       " ['hands', 14],\n",
       " ['happens', 2],\n",
       " ['happens', 3],\n",
       " ['hard', 4],\n",
       " ['headache', 13],\n",
       " ['health-care', 7],\n",
       " ['help', 7],\n",
       " ['higher', 12],\n",
       " ['house', 6],\n",
       " ['hygiene', 7],\n",
       " ['icc', 4],\n",
       " ['imagination', 4],\n",
       " ['important', 12],\n",
       " ['imposed', 8],\n",
       " ['inadequately', 12],\n",
       " ['inaugural', 4],\n",
       " ['include', 13],\n",
       " ['included', 7],\n",
       " ['india', 4],\n",
       " ['indian', 3],\n",
       " ['indian', 6],\n",
       " ['indian', 8],\n",
       " ['industry', 6],\n",
       " ['industry', 8],\n",
       " ['infected', 12],\n",
       " ['information', 1],\n",
       " ['information', 2],\n",
       " ['information', 2],\n",
       " ['information', 2],\n",
       " ['international', 3],\n",
       " ['introduced', 7],\n",
       " ['ipl', 3],\n",
       " ['ipl', 3],\n",
       " ['ipl', 5],\n",
       " ['ipl', 5],\n",
       " ['ipl', 5],\n",
       " ['jobless', 8],\n",
       " ['join', 11],\n",
       " ['joined', 11],\n",
       " ['kaprada', 10],\n",
       " ['kaprada', 11],\n",
       " ['keen', 4],\n",
       " ['keeping', 14],\n",
       " ['know', 2],\n",
       " ['large', 3],\n",
       " ['leader', 9],\n",
       " ['leaders', 11],\n",
       " ['league', 3],\n",
       " ['least', 8],\n",
       " ['less', 13],\n",
       " ['live', 14],\n",
       " ['livelihood', 8],\n",
       " ['local', 14],\n",
       " ['lockdown', 8],\n",
       " ['long', 12],\n",
       " ['loss', 13],\n",
       " ['lost', 8],\n",
       " ['love-hate', 5],\n",
       " ['major', 6],\n",
       " ['make', 10],\n",
       " ['mani', 4],\n",
       " ['many', 11],\n",
       " ['march', 3],\n",
       " ['march', 8],\n",
       " ['mark', 9],\n",
       " ['mask', 14],\n",
       " ['may', 13],\n",
       " ['may', 3],\n",
       " ['means', 8],\n",
       " ['measures', 7],\n",
       " ['might', 2],\n",
       " ['million', 8],\n",
       " ['minister', 9],\n",
       " ['ministers', 9],\n",
       " ['misunderstood', 2],\n",
       " ['modi', 9],\n",
       " ['moisture', 9],\n",
       " ['monitor', 7],\n",
       " ['mostly', 3],\n",
       " ['movie', 6],\n",
       " ['names', 11],\n",
       " ['narendra', 9],\n",
       " ['nasal', 13],\n",
       " ['necessary', 8],\n",
       " ['need', 2],\n",
       " ['need', 2],\n",
       " ['needed', 2],\n",
       " ['nevertheless', 2],\n",
       " ['next', 6],\n",
       " ['november', 10],\n",
       " ['officers', 7],\n",
       " ['officially', 11],\n",
       " ['often', 2],\n",
       " ['outbreakthe', 6],\n",
       " ['overt', 2],\n",
       " ['pains', 13],\n",
       " ['pandemic', 8],\n",
       " ['part', 3],\n",
       " ['parties', 11],\n",
       " ['party', 11],\n",
       " ['passed', 5],\n",
       " ['patients', 13],\n",
       " ['people', 12],\n",
       " ['period', 3],\n",
       " ['periods', 12],\n",
       " ['pet', 2],\n",
       " ['physical', 14],\n",
       " ['pick', 5],\n",
       " ['players', 5],\n",
       " ['possible', 9],\n",
       " ['practice', 2],\n",
       " ['precautions', 12],\n",
       " ['precautions', 14],\n",
       " ['precautions', 8],\n",
       " ['premier', 3],\n",
       " ['present', 2],\n",
       " ['president', 4],\n",
       " ['prevent', 7],\n",
       " ['prime', 9],\n",
       " ['processing', 1],\n",
       " ['producers', 8],\n",
       " ['production', 6],\n",
       " ['proximity', 12],\n",
       " ['public', 4],\n",
       " ['purchase', 2],\n",
       " ['python', 2],\n",
       " ['python', 2],\n",
       " ['query', 2],\n",
       " ['rahul', 9],\n",
       " ['rash', 13],\n",
       " ['really', 4],\n",
       " ['red', 13],\n",
       " ['relationship', 5],\n",
       " ['releases', 6],\n",
       " ['relevant', 2],\n",
       " ['rendered', 8],\n",
       " ['reported', 10],\n",
       " ['respective', 10],\n",
       " ['respiratory', 12],\n",
       " ['resume', 8],\n",
       " ['retrieval', 1],\n",
       " ['retrieved', 1],\n",
       " ['risks', 12],\n",
       " ['rival', 10],\n",
       " ['rolling', 6],\n",
       " ['rooms', 14],\n",
       " ['rupees', 8],\n",
       " ['safe', 14],\n",
       " ['said', 8],\n",
       " ['sanitization', 7],\n",
       " ['saying', 9],\n",
       " ['schedule', 7],\n",
       " ['search', 2],\n",
       " ['seats', 10],\n",
       " ['seats', 10],\n",
       " ['seats', 11],\n",
       " ['seems', 4],\n",
       " ['set', 1],\n",
       " ['set', 1],\n",
       " ['setting', 4],\n",
       " ['several', 6],\n",
       " ['shooting', 6],\n",
       " ['shooting', 7],\n",
       " ['short', 3],\n",
       " ['shutter', 6],\n",
       " ['simple', 14],\n",
       " ['since', 3],\n",
       " ['since', 8],\n",
       " ['skin', 13],\n",
       " ['slew', 6],\n",
       " ['smell', 13],\n",
       " ['sore', 13],\n",
       " ['south', 10],\n",
       " ['spaces', 12],\n",
       " ['spend', 12],\n",
       " ['spread', 7],\n",
       " ['spreading', 14],\n",
       " ['spreads', 12],\n",
       " ['standards', 7],\n",
       " ['stars', 3],\n",
       " ['stated', 2],\n",
       " ['stay', 14],\n",
       " ['strictest', 7],\n",
       " ['subtly', 4],\n",
       " ['suggesting', 9],\n",
       " ['supporting', 1],\n",
       " ['symptoms', 13],\n",
       " ['symptoms', 13],\n",
       " ['t20', 4],\n",
       " ['t20', 4],\n",
       " ['tactics', 10],\n",
       " ['take', 3],\n",
       " ['taking', 12],\n",
       " ['taking', 14],\n",
       " ['taking', 3],\n",
       " ['task', 1],\n",
       " ['taste', 13],\n",
       " ['team', 4],\n",
       " ['throat', 13],\n",
       " ['time', 12],\n",
       " ['time', 3],\n",
       " ['tiredness', 13],\n",
       " ['tissue', 14],\n",
       " ['together', 12],\n",
       " ['top', 3],\n",
       " ['turbines', 9],\n",
       " ['tweeted', 9],\n",
       " ['two', 11],\n",
       " ['two-month', 3],\n",
       " ['types', 2],\n",
       " ['uninformed', 9],\n",
       " ['use', 9],\n",
       " ['user', 2],\n",
       " ['users', 1],\n",
       " ['using', 10],\n",
       " ['usually', 5],\n",
       " ['ventilated', 12],\n",
       " ['ventilated', 14],\n",
       " ['virus', 12],\n",
       " ['virus', 6],\n",
       " ['virus', 7],\n",
       " ['virus', 8],\n",
       " ['wanting', 2],\n",
       " ['war', 9],\n",
       " ['wave', 10],\n",
       " ['wearing', 14],\n",
       " ['web', 2],\n",
       " ['well', 14],\n",
       " ['wind', 9],\n",
       " ['window', 3],\n",
       " ['words', 2],\n",
       " ['words', 9],\n",
       " ['work', 14],\n",
       " ['work', 8],\n",
       " ['workers', 11],\n",
       " ['world', 3],\n",
       " ['world', 3],\n",
       " ['world', 4],\n",
       " ['year', 6],\n",
       " ['—', 5],\n",
       " ['’', 3]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_without_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Filtering distinct words from Tokenized words\n",
    "\n",
    "all_items = [i[0] for i in token_without_stopwords]\n",
    "distinct_item = []\n",
    "for i in token_without_stopwords:\n",
    "    if i[0] not in distinct_item:\n",
    "        distinct_item.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generating pairs of Distinct item with its overall count\n",
    "\n",
    "distinct_item_with_count=[]\n",
    "distinct_item_with_count_and_docs=[]\n",
    "for i in distinct_item:\n",
    "    distinct_item_with_count.append([i,all_items.count(i)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "323"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(distinct_item_with_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['$', 1],\n",
       " ['(', 2],\n",
       " [')', 2],\n",
       " ['1000', 1],\n",
       " ['133', 1],\n",
       " ['2008', 1],\n",
       " ['3', 1],\n",
       " ['aches', 1],\n",
       " ['addresses', 1],\n",
       " ['advantage', 1],\n",
       " ['advice', 1],\n",
       " ['aerosols', 1],\n",
       " ['affect', 1],\n",
       " ['ahead', 1],\n",
       " ['allegedly', 1],\n",
       " ['also', 2],\n",
       " ['announced', 1],\n",
       " ['appears', 1],\n",
       " ['appointing', 1],\n",
       " ['arm-twisted', 1],\n",
       " ['assembly', 2],\n",
       " ['atmosphere', 1],\n",
       " ['avoiding', 1],\n",
       " ['based', 1],\n",
       " ['believe', 1],\n",
       " ['bell', 1],\n",
       " ['bent', 1],\n",
       " ['big', 1],\n",
       " ['bjp', 4],\n",
       " ['break', 1],\n",
       " ['broke', 1],\n",
       " ['browsing', 1],\n",
       " ['bypolls', 1],\n",
       " ['camps', 1],\n",
       " ['candidate', 1],\n",
       " ['candidates', 1],\n",
       " ['captured', 1],\n",
       " ['cat', 1],\n",
       " ['check', 1],\n",
       " ['cleaning', 1],\n",
       " ['close', 1],\n",
       " ['clustering', 1],\n",
       " ['collections', 1],\n",
       " ['collective', 1],\n",
       " ['coming', 1],\n",
       " ['comment', 1],\n",
       " ['commitments', 1],\n",
       " ['common', 2],\n",
       " ['community', 1],\n",
       " ['congestion', 1],\n",
       " ['congress', 5],\n",
       " ['contain', 1],\n",
       " ['contents', 1],\n",
       " ['cough', 1],\n",
       " ['coughing', 1],\n",
       " ['covers', 1],\n",
       " ['covid', 1],\n",
       " ['covid-19', 3],\n",
       " ['cricket', 1],\n",
       " ['cricketing', 3],\n",
       " ['crore', 1],\n",
       " ['crowded', 1],\n",
       " ['crowds', 1],\n",
       " ['cynosure', 1],\n",
       " ['dangs', 1],\n",
       " ['decades', 1],\n",
       " ['decided', 1],\n",
       " ['defections', 1],\n",
       " ['dent', 1],\n",
       " ['designated', 1],\n",
       " ['diarrhoea', 1],\n",
       " ['distancing', 1],\n",
       " ['distinction', 1],\n",
       " ['document', 2],\n",
       " ['documents', 3],\n",
       " ['domestic', 2],\n",
       " ['doubt', 1],\n",
       " ['draw', 1],\n",
       " ['droplets', 1],\n",
       " ['dry', 1],\n",
       " ['efficiently', 1],\n",
       " ['ehsan', 1],\n",
       " ['eight', 1],\n",
       " ['either', 1],\n",
       " ['elbow', 1],\n",
       " ['engine', 1],\n",
       " ['english', 2],\n",
       " ['ensure', 1],\n",
       " ['entertainment', 2],\n",
       " ['entirely', 1],\n",
       " ['environments', 1],\n",
       " ['especially', 1],\n",
       " ['establishment', 1],\n",
       " ['estimated', 1],\n",
       " ['even', 1],\n",
       " ['every', 1],\n",
       " ['excel', 1],\n",
       " ['experts', 1],\n",
       " ['explains', 1],\n",
       " ['eyes', 2],\n",
       " ['fever', 1],\n",
       " ['field', 2],\n",
       " ['fielding', 1],\n",
       " ['fighting', 1],\n",
       " ['filmmakers', 1],\n",
       " ['filtering', 1],\n",
       " ['first', 1],\n",
       " ['first-mover', 1],\n",
       " ['forced', 1],\n",
       " ['format', 1],\n",
       " ['former', 1],\n",
       " ['future', 1],\n",
       " ['gandhi', 1],\n",
       " ['getting', 1],\n",
       " ['given', 1],\n",
       " ['good', 1],\n",
       " ['grouping', 1],\n",
       " ['gujarat', 2],\n",
       " ['hands', 1],\n",
       " ['happens', 2],\n",
       " ['hard', 1],\n",
       " ['headache', 1],\n",
       " ['health-care', 1],\n",
       " ['help', 1],\n",
       " ['higher', 1],\n",
       " ['house', 1],\n",
       " ['hygiene', 1],\n",
       " ['icc', 1],\n",
       " ['imagination', 1],\n",
       " ['important', 1],\n",
       " ['imposed', 1],\n",
       " ['inadequately', 1],\n",
       " ['inaugural', 1],\n",
       " ['include', 1],\n",
       " ['included', 1],\n",
       " ['india', 1],\n",
       " ['indian', 3],\n",
       " ['industry', 2],\n",
       " ['infected', 1],\n",
       " ['information', 4],\n",
       " ['international', 1],\n",
       " ['introduced', 1],\n",
       " ['ipl', 5],\n",
       " ['jobless', 1],\n",
       " ['join', 1],\n",
       " ['joined', 1],\n",
       " ['kaprada', 2],\n",
       " ['keen', 1],\n",
       " ['keeping', 1],\n",
       " ['know', 1],\n",
       " ['large', 1],\n",
       " ['leader', 1],\n",
       " ['leaders', 1],\n",
       " ['league', 1],\n",
       " ['least', 1],\n",
       " ['less', 1],\n",
       " ['live', 1],\n",
       " ['livelihood', 1],\n",
       " ['local', 1],\n",
       " ['lockdown', 1],\n",
       " ['long', 1],\n",
       " ['loss', 1],\n",
       " ['lost', 1],\n",
       " ['love-hate', 1],\n",
       " ['major', 1],\n",
       " ['make', 1],\n",
       " ['mani', 1],\n",
       " ['many', 1],\n",
       " ['march', 2],\n",
       " ['mark', 1],\n",
       " ['mask', 1],\n",
       " ['may', 2],\n",
       " ['means', 1],\n",
       " ['measures', 1],\n",
       " ['might', 1],\n",
       " ['million', 1],\n",
       " ['minister', 1],\n",
       " ['ministers', 1],\n",
       " ['misunderstood', 1],\n",
       " ['modi', 1],\n",
       " ['moisture', 1],\n",
       " ['monitor', 1],\n",
       " ['mostly', 1],\n",
       " ['movie', 1],\n",
       " ['names', 1],\n",
       " ['narendra', 1],\n",
       " ['nasal', 1],\n",
       " ['necessary', 1],\n",
       " ['need', 2],\n",
       " ['needed', 1],\n",
       " ['nevertheless', 1],\n",
       " ['next', 1],\n",
       " ['november', 1],\n",
       " ['officers', 1],\n",
       " ['officially', 1],\n",
       " ['often', 1],\n",
       " ['outbreakthe', 1],\n",
       " ['overt', 1],\n",
       " ['pains', 1],\n",
       " ['pandemic', 1],\n",
       " ['part', 1],\n",
       " ['parties', 1],\n",
       " ['party', 1],\n",
       " ['passed', 1],\n",
       " ['patients', 1],\n",
       " ['people', 1],\n",
       " ['period', 1],\n",
       " ['periods', 1],\n",
       " ['pet', 1],\n",
       " ['physical', 1],\n",
       " ['pick', 1],\n",
       " ['players', 1],\n",
       " ['possible', 1],\n",
       " ['practice', 1],\n",
       " ['precautions', 3],\n",
       " ['premier', 1],\n",
       " ['present', 1],\n",
       " ['president', 1],\n",
       " ['prevent', 1],\n",
       " ['prime', 1],\n",
       " ['processing', 1],\n",
       " ['producers', 1],\n",
       " ['production', 1],\n",
       " ['proximity', 1],\n",
       " ['public', 1],\n",
       " ['purchase', 1],\n",
       " ['python', 2],\n",
       " ['query', 1],\n",
       " ['rahul', 1],\n",
       " ['rash', 1],\n",
       " ['really', 1],\n",
       " ['red', 1],\n",
       " ['relationship', 1],\n",
       " ['releases', 1],\n",
       " ['relevant', 1],\n",
       " ['rendered', 1],\n",
       " ['reported', 1],\n",
       " ['respective', 1],\n",
       " ['respiratory', 1],\n",
       " ['resume', 1],\n",
       " ['retrieval', 1],\n",
       " ['retrieved', 1],\n",
       " ['risks', 1],\n",
       " ['rival', 1],\n",
       " ['rolling', 1],\n",
       " ['rooms', 1],\n",
       " ['rupees', 1],\n",
       " ['safe', 1],\n",
       " ['said', 1],\n",
       " ['sanitization', 1],\n",
       " ['saying', 1],\n",
       " ['schedule', 1],\n",
       " ['search', 1],\n",
       " ['seats', 3],\n",
       " ['seems', 1],\n",
       " ['set', 2],\n",
       " ['setting', 1],\n",
       " ['several', 1],\n",
       " ['shooting', 2],\n",
       " ['short', 1],\n",
       " ['shutter', 1],\n",
       " ['simple', 1],\n",
       " ['since', 2],\n",
       " ['skin', 1],\n",
       " ['slew', 1],\n",
       " ['smell', 1],\n",
       " ['sore', 1],\n",
       " ['south', 1],\n",
       " ['spaces', 1],\n",
       " ['spend', 1],\n",
       " ['spread', 1],\n",
       " ['spreading', 1],\n",
       " ['spreads', 1],\n",
       " ['standards', 1],\n",
       " ['stars', 1],\n",
       " ['stated', 1],\n",
       " ['stay', 1],\n",
       " ['strictest', 1],\n",
       " ['subtly', 1],\n",
       " ['suggesting', 1],\n",
       " ['supporting', 1],\n",
       " ['symptoms', 2],\n",
       " ['t20', 2],\n",
       " ['tactics', 1],\n",
       " ['take', 1],\n",
       " ['taking', 3],\n",
       " ['task', 1],\n",
       " ['taste', 1],\n",
       " ['team', 1],\n",
       " ['throat', 1],\n",
       " ['time', 2],\n",
       " ['tiredness', 1],\n",
       " ['tissue', 1],\n",
       " ['together', 1],\n",
       " ['top', 1],\n",
       " ['turbines', 1],\n",
       " ['tweeted', 1],\n",
       " ['two', 1],\n",
       " ['two-month', 1],\n",
       " ['types', 1],\n",
       " ['uninformed', 1],\n",
       " ['use', 1],\n",
       " ['user', 1],\n",
       " ['users', 1],\n",
       " ['using', 1],\n",
       " ['usually', 1],\n",
       " ['ventilated', 2],\n",
       " ['virus', 4],\n",
       " ['wanting', 1],\n",
       " ['war', 1],\n",
       " ['wave', 1],\n",
       " ['wearing', 1],\n",
       " ['web', 1],\n",
       " ['well', 1],\n",
       " ['wind', 1],\n",
       " ['window', 1],\n",
       " ['words', 2],\n",
       " ['work', 2],\n",
       " ['workers', 1],\n",
       " ['world', 3],\n",
       " ['year', 1],\n",
       " ['—', 1],\n",
       " ['’', 1]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distinct_item_with_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generating pairs of Distinct item with its overall count and its posting list\n",
    "\n",
    "for i in range(len(distinct_item)):\n",
    "    docs_list = []\n",
    "    for j in range(len(token_without_stopwords)):\n",
    "        if(distinct_item[i] == token_without_stopwords[j][0]):\n",
    "            docs_list.append(token_without_stopwords[j][1])\n",
    "    docs_list.sort()\n",
    "    docs_list= list(unique_everseen(docs_list))\n",
    "    if(distinct_item_with_count[i][0] == distinct_item[i] and distinct_item_with_count[i][1] == len(docs_list)):\n",
    "        distinct_item_with_count_and_docs.append([distinct_item_with_count[i][0],distinct_item_with_count[i][1],docs_list])\n",
    "    \n",
    "    else:\n",
    "        if(distinct_item_with_count[i][0] == distinct_item[i] and distinct_item_with_count[i][1] > len(docs_list)):\n",
    "            distinct_item_with_count_and_docs.append([distinct_item_with_count[i][0],len(docs_list),docs_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['$', 1, [8]],\n",
       " ['(', 2, [3, 8]],\n",
       " [')', 2, [3, 8]],\n",
       " ['1000', 1, [8]],\n",
       " ['133', 1, [8]],\n",
       " ['2008', 1, [3]],\n",
       " ['3', 1, [10]],\n",
       " ['aches', 1, [13]],\n",
       " ['addresses', 1, [2]],\n",
       " ['advantage', 1, [5]]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Printing Top 10 records\n",
    "\n",
    "distinct_item_with_count_and_docs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverted Index :-\n",
      "-------------------------------------------------------------\n",
      "Term\t\tFrequency Count\t\tPostings Lists\n",
      "$ \t\t\t 1 \t\t\t 8\n",
      "( \t\t\t 2 \t\t\t 3 => 8\n",
      ") \t\t\t 2 \t\t\t 3 => 8\n",
      "1000 \t\t\t 1 \t\t\t 8\n",
      "133 \t\t\t 1 \t\t\t 8\n",
      "2008 \t\t\t 1 \t\t\t 3\n",
      "3 \t\t\t 1 \t\t\t 10\n",
      "aches \t\t\t 1 \t\t\t 13\n",
      "addresses \t\t\t 1 \t\t\t 2\n",
      "advantage \t\t\t 1 \t\t\t 5\n",
      "advice \t\t\t 1 \t\t\t 14\n",
      "aerosols \t\t\t 1 \t\t\t 12\n",
      "affect \t\t\t 1 \t\t\t 13\n",
      "ahead \t\t\t 1 \t\t\t 10\n",
      "allegedly \t\t\t 1 \t\t\t 10\n",
      "also \t\t\t 2 \t\t\t 1 => 5\n",
      "announced \t\t\t 1 \t\t\t 11\n",
      "appears \t\t\t 1 \t\t\t 12\n",
      "appointing \t\t\t 1 \t\t\t 7\n",
      "arm-twisted \t\t\t 1 \t\t\t 4\n",
      "assembly \t\t\t 1 \t\t\t 10\n",
      "atmosphere \t\t\t 1 \t\t\t 9\n",
      "avoiding \t\t\t 1 \t\t\t 14\n",
      "based \t\t\t 1 \t\t\t 1\n",
      "believe \t\t\t 1 \t\t\t 4\n",
      "bell \t\t\t 1 \t\t\t 6\n",
      "bent \t\t\t 1 \t\t\t 14\n",
      "big \t\t\t 1 \t\t\t 6\n",
      "bjp \t\t\t 3 \t\t\t 9 => 10 => 11\n",
      "break \t\t\t 1 \t\t\t 3\n",
      "broke \t\t\t 1 \t\t\t 9\n",
      "browsing \t\t\t 1 \t\t\t 1\n",
      "bypolls \t\t\t 1 \t\t\t 10\n",
      "camps \t\t\t 1 \t\t\t 10\n",
      "candidate \t\t\t 1 \t\t\t 11\n",
      "candidates \t\t\t 1 \t\t\t 11\n",
      "captured \t\t\t 1 \t\t\t 4\n",
      "cat \t\t\t 1 \t\t\t 6\n",
      "check \t\t\t 1 \t\t\t 14\n",
      "cleaning \t\t\t 1 \t\t\t 14\n",
      "close \t\t\t 1 \t\t\t 12\n",
      "clustering \t\t\t 1 \t\t\t 1\n",
      "collections \t\t\t 1 \t\t\t 1\n",
      "collective \t\t\t 1 \t\t\t 3\n",
      "coming \t\t\t 1 \t\t\t 1\n",
      "comment \t\t\t 1 \t\t\t 9\n",
      "commitments \t\t\t 1 \t\t\t 5\n",
      "common \t\t\t 1 \t\t\t 13\n",
      "community \t\t\t 1 \t\t\t 14\n",
      "congestion \t\t\t 1 \t\t\t 13\n",
      "congress \t\t\t 3 \t\t\t 9 => 10 => 11\n",
      "contain \t\t\t 1 \t\t\t 2\n",
      "contents \t\t\t 1 \t\t\t 1\n",
      "cough \t\t\t 1 \t\t\t 13\n",
      "coughing \t\t\t 1 \t\t\t 14\n",
      "covers \t\t\t 1 \t\t\t 1\n",
      "covid \t\t\t 1 \t\t\t 7\n",
      "covid-19 \t\t\t 3 \t\t\t 12 => 13 => 14\n",
      "cricket \t\t\t 1 \t\t\t 3\n",
      "cricketing \t\t\t 2 \t\t\t 3 => 5\n",
      "crore \t\t\t 1 \t\t\t 8\n",
      "crowded \t\t\t 1 \t\t\t 12\n",
      "crowds \t\t\t 1 \t\t\t 14\n",
      "cynosure \t\t\t 1 \t\t\t 3\n",
      "dangs \t\t\t 1 \t\t\t 10\n",
      "decades \t\t\t 1 \t\t\t 11\n",
      "decided \t\t\t 1 \t\t\t 11\n",
      "defections \t\t\t 1 \t\t\t 10\n",
      "dent \t\t\t 1 \t\t\t 10\n",
      "designated \t\t\t 1 \t\t\t 7\n",
      "diarrhoea \t\t\t 1 \t\t\t 13\n",
      "distancing \t\t\t 1 \t\t\t 14\n",
      "distinction \t\t\t 1 \t\t\t 2\n",
      "document \t\t\t 2 \t\t\t 1 => 2\n",
      "documents \t\t\t 1 \t\t\t 1\n",
      "domestic \t\t\t 2 \t\t\t 3 => 5\n",
      "doubt \t\t\t 1 \t\t\t 5\n",
      "draw \t\t\t 1 \t\t\t 9\n",
      "droplets \t\t\t 1 \t\t\t 12\n",
      "dry \t\t\t 1 \t\t\t 13\n",
      "efficiently \t\t\t 1 \t\t\t 12\n",
      "ehsan \t\t\t 1 \t\t\t 4\n",
      "eight \t\t\t 1 \t\t\t 10\n",
      "either \t\t\t 1 \t\t\t 5\n",
      "elbow \t\t\t 1 \t\t\t 14\n",
      "engine \t\t\t 1 \t\t\t 2\n",
      "english \t\t\t 1 \t\t\t 5\n",
      "ensure \t\t\t 1 \t\t\t 7\n",
      "entertainment \t\t\t 2 \t\t\t 6 => 8\n",
      "entirely \t\t\t 1 \t\t\t 5\n",
      "environments \t\t\t 1 \t\t\t 12\n",
      "especially \t\t\t 1 \t\t\t 8\n",
      "establishment \t\t\t 1 \t\t\t 5\n",
      "estimated \t\t\t 1 \t\t\t 8\n",
      "even \t\t\t 1 \t\t\t 12\n",
      "every \t\t\t 1 \t\t\t 7\n",
      "excel \t\t\t 1 \t\t\t 6\n",
      "experts \t\t\t 1 \t\t\t 7\n",
      "explains \t\t\t 1 \t\t\t 5\n",
      "eyes \t\t\t 2 \t\t\t 3 => 13\n",
      "fever \t\t\t 1 \t\t\t 13\n",
      "field \t\t\t 2 \t\t\t 1 => 11\n",
      "fielding \t\t\t 1 \t\t\t 4\n",
      "fighting \t\t\t 1 \t\t\t 11\n",
      "filmmakers \t\t\t 1 \t\t\t 6\n",
      "filtering \t\t\t 1 \t\t\t 1\n",
      "first \t\t\t 1 \t\t\t 6\n",
      "first-mover \t\t\t 1 \t\t\t 5\n",
      "forced \t\t\t 1 \t\t\t 6\n",
      "format \t\t\t 1 \t\t\t 4\n",
      "former \t\t\t 1 \t\t\t 4\n",
      "future \t\t\t 1 \t\t\t 11\n",
      "gandhi \t\t\t 1 \t\t\t 9\n",
      "getting \t\t\t 1 \t\t\t 12\n",
      "given \t\t\t 1 \t\t\t 1\n",
      "good \t\t\t 1 \t\t\t 1\n",
      "grouping \t\t\t 1 \t\t\t 1\n",
      "gujarat \t\t\t 1 \t\t\t 10\n",
      "hands \t\t\t 1 \t\t\t 14\n",
      "happens \t\t\t 2 \t\t\t 2 => 3\n",
      "hard \t\t\t 1 \t\t\t 4\n",
      "headache \t\t\t 1 \t\t\t 13\n",
      "health-care \t\t\t 1 \t\t\t 7\n",
      "help \t\t\t 1 \t\t\t 7\n",
      "higher \t\t\t 1 \t\t\t 12\n",
      "house \t\t\t 1 \t\t\t 6\n",
      "hygiene \t\t\t 1 \t\t\t 7\n",
      "icc \t\t\t 1 \t\t\t 4\n",
      "imagination \t\t\t 1 \t\t\t 4\n",
      "important \t\t\t 1 \t\t\t 12\n",
      "imposed \t\t\t 1 \t\t\t 8\n",
      "inadequately \t\t\t 1 \t\t\t 12\n",
      "inaugural \t\t\t 1 \t\t\t 4\n",
      "include \t\t\t 1 \t\t\t 13\n",
      "included \t\t\t 1 \t\t\t 7\n",
      "india \t\t\t 1 \t\t\t 4\n",
      "indian \t\t\t 3 \t\t\t 3 => 6 => 8\n",
      "industry \t\t\t 2 \t\t\t 6 => 8\n",
      "infected \t\t\t 1 \t\t\t 12\n",
      "information \t\t\t 2 \t\t\t 1 => 2\n",
      "international \t\t\t 1 \t\t\t 3\n",
      "introduced \t\t\t 1 \t\t\t 7\n",
      "ipl \t\t\t 2 \t\t\t 3 => 5\n",
      "jobless \t\t\t 1 \t\t\t 8\n",
      "join \t\t\t 1 \t\t\t 11\n",
      "joined \t\t\t 1 \t\t\t 11\n",
      "kaprada \t\t\t 2 \t\t\t 10 => 11\n",
      "keen \t\t\t 1 \t\t\t 4\n",
      "keeping \t\t\t 1 \t\t\t 14\n",
      "know \t\t\t 1 \t\t\t 2\n",
      "large \t\t\t 1 \t\t\t 3\n",
      "leader \t\t\t 1 \t\t\t 9\n",
      "leaders \t\t\t 1 \t\t\t 11\n",
      "league \t\t\t 1 \t\t\t 3\n",
      "least \t\t\t 1 \t\t\t 8\n",
      "less \t\t\t 1 \t\t\t 13\n",
      "live \t\t\t 1 \t\t\t 14\n",
      "livelihood \t\t\t 1 \t\t\t 8\n",
      "local \t\t\t 1 \t\t\t 14\n",
      "lockdown \t\t\t 1 \t\t\t 8\n",
      "long \t\t\t 1 \t\t\t 12\n",
      "loss \t\t\t 1 \t\t\t 13\n",
      "lost \t\t\t 1 \t\t\t 8\n",
      "love-hate \t\t\t 1 \t\t\t 5\n",
      "major \t\t\t 1 \t\t\t 6\n",
      "make \t\t\t 1 \t\t\t 10\n",
      "mani \t\t\t 1 \t\t\t 4\n",
      "many \t\t\t 1 \t\t\t 11\n",
      "march \t\t\t 2 \t\t\t 3 => 8\n",
      "mark \t\t\t 1 \t\t\t 9\n",
      "mask \t\t\t 1 \t\t\t 14\n",
      "may \t\t\t 2 \t\t\t 3 => 13\n",
      "means \t\t\t 1 \t\t\t 8\n",
      "measures \t\t\t 1 \t\t\t 7\n",
      "might \t\t\t 1 \t\t\t 2\n",
      "million \t\t\t 1 \t\t\t 8\n",
      "minister \t\t\t 1 \t\t\t 9\n",
      "ministers \t\t\t 1 \t\t\t 9\n",
      "misunderstood \t\t\t 1 \t\t\t 2\n",
      "modi \t\t\t 1 \t\t\t 9\n",
      "moisture \t\t\t 1 \t\t\t 9\n",
      "monitor \t\t\t 1 \t\t\t 7\n",
      "mostly \t\t\t 1 \t\t\t 3\n",
      "movie \t\t\t 1 \t\t\t 6\n",
      "names \t\t\t 1 \t\t\t 11\n",
      "narendra \t\t\t 1 \t\t\t 9\n",
      "nasal \t\t\t 1 \t\t\t 13\n",
      "necessary \t\t\t 1 \t\t\t 8\n",
      "need \t\t\t 1 \t\t\t 2\n",
      "needed \t\t\t 1 \t\t\t 2\n",
      "nevertheless \t\t\t 1 \t\t\t 2\n",
      "next \t\t\t 1 \t\t\t 6\n",
      "november \t\t\t 1 \t\t\t 10\n",
      "officers \t\t\t 1 \t\t\t 7\n",
      "officially \t\t\t 1 \t\t\t 11\n",
      "often \t\t\t 1 \t\t\t 2\n",
      "outbreakthe \t\t\t 1 \t\t\t 6\n",
      "overt \t\t\t 1 \t\t\t 2\n",
      "pains \t\t\t 1 \t\t\t 13\n",
      "pandemic \t\t\t 1 \t\t\t 8\n",
      "part \t\t\t 1 \t\t\t 3\n",
      "parties \t\t\t 1 \t\t\t 11\n",
      "party \t\t\t 1 \t\t\t 11\n",
      "passed \t\t\t 1 \t\t\t 5\n",
      "patients \t\t\t 1 \t\t\t 13\n",
      "people \t\t\t 1 \t\t\t 12\n",
      "period \t\t\t 1 \t\t\t 3\n",
      "periods \t\t\t 1 \t\t\t 12\n",
      "pet \t\t\t 1 \t\t\t 2\n",
      "physical \t\t\t 1 \t\t\t 14\n",
      "pick \t\t\t 1 \t\t\t 5\n",
      "players \t\t\t 1 \t\t\t 5\n",
      "possible \t\t\t 1 \t\t\t 9\n",
      "practice \t\t\t 1 \t\t\t 2\n",
      "precautions \t\t\t 3 \t\t\t 8 => 12 => 14\n",
      "premier \t\t\t 1 \t\t\t 3\n",
      "present \t\t\t 1 \t\t\t 2\n",
      "president \t\t\t 1 \t\t\t 4\n",
      "prevent \t\t\t 1 \t\t\t 7\n",
      "prime \t\t\t 1 \t\t\t 9\n",
      "processing \t\t\t 1 \t\t\t 1\n",
      "producers \t\t\t 1 \t\t\t 8\n",
      "production \t\t\t 1 \t\t\t 6\n",
      "proximity \t\t\t 1 \t\t\t 12\n",
      "public \t\t\t 1 \t\t\t 4\n",
      "purchase \t\t\t 1 \t\t\t 2\n",
      "python \t\t\t 1 \t\t\t 2\n",
      "query \t\t\t 1 \t\t\t 2\n",
      "rahul \t\t\t 1 \t\t\t 9\n",
      "rash \t\t\t 1 \t\t\t 13\n",
      "really \t\t\t 1 \t\t\t 4\n",
      "red \t\t\t 1 \t\t\t 13\n",
      "relationship \t\t\t 1 \t\t\t 5\n",
      "releases \t\t\t 1 \t\t\t 6\n",
      "relevant \t\t\t 1 \t\t\t 2\n",
      "rendered \t\t\t 1 \t\t\t 8\n",
      "reported \t\t\t 1 \t\t\t 10\n",
      "respective \t\t\t 1 \t\t\t 10\n",
      "respiratory \t\t\t 1 \t\t\t 12\n",
      "resume \t\t\t 1 \t\t\t 8\n",
      "retrieval \t\t\t 1 \t\t\t 1\n",
      "retrieved \t\t\t 1 \t\t\t 1\n",
      "risks \t\t\t 1 \t\t\t 12\n",
      "rival \t\t\t 1 \t\t\t 10\n",
      "rolling \t\t\t 1 \t\t\t 6\n",
      "rooms \t\t\t 1 \t\t\t 14\n",
      "rupees \t\t\t 1 \t\t\t 8\n",
      "safe \t\t\t 1 \t\t\t 14\n",
      "said \t\t\t 1 \t\t\t 8\n",
      "sanitization \t\t\t 1 \t\t\t 7\n",
      "saying \t\t\t 1 \t\t\t 9\n",
      "schedule \t\t\t 1 \t\t\t 7\n",
      "search \t\t\t 1 \t\t\t 2\n",
      "seats \t\t\t 2 \t\t\t 10 => 11\n",
      "seems \t\t\t 1 \t\t\t 4\n",
      "set \t\t\t 1 \t\t\t 1\n",
      "setting \t\t\t 1 \t\t\t 4\n",
      "several \t\t\t 1 \t\t\t 6\n",
      "shooting \t\t\t 2 \t\t\t 6 => 7\n",
      "short \t\t\t 1 \t\t\t 3\n",
      "shutter \t\t\t 1 \t\t\t 6\n",
      "simple \t\t\t 1 \t\t\t 14\n",
      "since \t\t\t 2 \t\t\t 3 => 8\n",
      "skin \t\t\t 1 \t\t\t 13\n",
      "slew \t\t\t 1 \t\t\t 6\n",
      "smell \t\t\t 1 \t\t\t 13\n",
      "sore \t\t\t 1 \t\t\t 13\n",
      "south \t\t\t 1 \t\t\t 10\n",
      "spaces \t\t\t 1 \t\t\t 12\n",
      "spend \t\t\t 1 \t\t\t 12\n",
      "spread \t\t\t 1 \t\t\t 7\n",
      "spreading \t\t\t 1 \t\t\t 14\n",
      "spreads \t\t\t 1 \t\t\t 12\n",
      "standards \t\t\t 1 \t\t\t 7\n",
      "stars \t\t\t 1 \t\t\t 3\n",
      "stated \t\t\t 1 \t\t\t 2\n",
      "stay \t\t\t 1 \t\t\t 14\n",
      "strictest \t\t\t 1 \t\t\t 7\n",
      "subtly \t\t\t 1 \t\t\t 4\n",
      "suggesting \t\t\t 1 \t\t\t 9\n",
      "supporting \t\t\t 1 \t\t\t 1\n",
      "symptoms \t\t\t 1 \t\t\t 13\n",
      "t20 \t\t\t 1 \t\t\t 4\n",
      "tactics \t\t\t 1 \t\t\t 10\n",
      "take \t\t\t 1 \t\t\t 3\n",
      "taking \t\t\t 3 \t\t\t 3 => 12 => 14\n",
      "task \t\t\t 1 \t\t\t 1\n",
      "taste \t\t\t 1 \t\t\t 13\n",
      "team \t\t\t 1 \t\t\t 4\n",
      "throat \t\t\t 1 \t\t\t 13\n",
      "time \t\t\t 2 \t\t\t 3 => 12\n",
      "tiredness \t\t\t 1 \t\t\t 13\n",
      "tissue \t\t\t 1 \t\t\t 14\n",
      "together \t\t\t 1 \t\t\t 12\n",
      "top \t\t\t 1 \t\t\t 3\n",
      "turbines \t\t\t 1 \t\t\t 9\n",
      "tweeted \t\t\t 1 \t\t\t 9\n",
      "two \t\t\t 1 \t\t\t 11\n",
      "two-month \t\t\t 1 \t\t\t 3\n",
      "types \t\t\t 1 \t\t\t 2\n",
      "uninformed \t\t\t 1 \t\t\t 9\n",
      "use \t\t\t 1 \t\t\t 9\n",
      "user \t\t\t 1 \t\t\t 2\n",
      "users \t\t\t 1 \t\t\t 1\n",
      "using \t\t\t 1 \t\t\t 10\n",
      "usually \t\t\t 1 \t\t\t 5\n",
      "ventilated \t\t\t 2 \t\t\t 12 => 14\n",
      "virus \t\t\t 4 \t\t\t 6 => 7 => 8 => 12\n",
      "wanting \t\t\t 1 \t\t\t 2\n",
      "war \t\t\t 1 \t\t\t 9\n",
      "wave \t\t\t 1 \t\t\t 10\n",
      "wearing \t\t\t 1 \t\t\t 14\n",
      "web \t\t\t 1 \t\t\t 2\n",
      "well \t\t\t 1 \t\t\t 14\n",
      "wind \t\t\t 1 \t\t\t 9\n",
      "window \t\t\t 1 \t\t\t 3\n",
      "words \t\t\t 2 \t\t\t 2 => 9\n",
      "work \t\t\t 2 \t\t\t 8 => 14\n",
      "workers \t\t\t 1 \t\t\t 11\n",
      "world \t\t\t 2 \t\t\t 3 => 4\n",
      "year \t\t\t 1 \t\t\t 6\n",
      "— \t\t\t 1 \t\t\t 5\n",
      "’ \t\t\t 1 \t\t\t 3\n"
     ]
    }
   ],
   "source": [
    "### Printing Inverted Index\n",
    "\n",
    "print(\"Inverted Index :-\")\n",
    "print(\"-------------------------------------------------------------\")\n",
    "print(\"Term\\t\\tFrequency Count\\t\\tPostings Lists\")\n",
    "for i in distinct_item_with_count_and_docs:\n",
    "    postings_lists = \" => \".join([str(k) for k in i[2]])\n",
    "    print(i[0],\"\\t\\t\\t\",i[1],\"\\t\\t\\t\",postings_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Documents Similarity ######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter query: Symptoms of Covid-19 is spreading\n"
     ]
    }
   ],
   "source": [
    "### Taking user input Query\n",
    "\n",
    "query = input(\"Please enter query: \").split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['symptoms', 'covid-19', 'spreading']"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = removing_stop_words(query)\n",
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(token_without_stopwords_docs_wise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['field',\n",
       "  'information',\n",
       "  'retrieval',\n",
       "  'also',\n",
       "  'covers',\n",
       "  'supporting',\n",
       "  'users',\n",
       "  'browsing',\n",
       "  'filtering',\n",
       "  'document',\n",
       "  'collections',\n",
       "  'processing',\n",
       "  'set',\n",
       "  'retrieved',\n",
       "  'documents',\n",
       "  'given',\n",
       "  'set',\n",
       "  'documents',\n",
       "  'clustering',\n",
       "  'task',\n",
       "  'coming',\n",
       "  'good',\n",
       "  'grouping',\n",
       "  'documents',\n",
       "  'based',\n",
       "  'contents'],\n",
       " 1]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_without_stopwords_docs_wise[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generating documents wise Items and its count pair\n",
    "\n",
    "items_with_frequency = []\n",
    "for i in range(len(token_without_stopwords_docs_wise)):\n",
    "    item = [k for k in token_without_stopwords_docs_wise[i][0]]\n",
    "    distinct_item1 = []\n",
    "    for j in item:\n",
    "        if j not in distinct_item1:\n",
    "            distinct_item1.append(j)\n",
    "            \n",
    "    var2 = []\n",
    "    for j in distinct_item1:\n",
    "        var1 = item.count(j)\n",
    "        var2.append([j,var1])\n",
    "    items_with_frequency.append([var2,token_without_stopwords_docs_wise[i][1]])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['field', 1],\n",
       "  ['information', 1],\n",
       "  ['retrieval', 1],\n",
       "  ['also', 1],\n",
       "  ['covers', 1],\n",
       "  ['supporting', 1],\n",
       "  ['users', 1],\n",
       "  ['browsing', 1],\n",
       "  ['filtering', 1],\n",
       "  ['document', 1],\n",
       "  ['collections', 1],\n",
       "  ['processing', 1],\n",
       "  ['set', 2],\n",
       "  ['retrieved', 1],\n",
       "  ['documents', 3],\n",
       "  ['given', 1],\n",
       "  ['clustering', 1],\n",
       "  ['task', 1],\n",
       "  ['coming', 1],\n",
       "  ['good', 1],\n",
       "  ['grouping', 1],\n",
       "  ['based', 1],\n",
       "  ['contents', 1]],\n",
       " 1]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items_with_frequency[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculating tf values for all given documents\n",
    "\n",
    "def tf_value(items_with_frequency):\n",
    "    tf_list = []\n",
    "    for i in items_with_frequency:\n",
    "        var3 =[]\n",
    "        for j in i[0]:\n",
    "            tf_val = j[1]/len(i[0])\n",
    "            var3.append([j[0],tf_val])\n",
    "        tf_list.append([var3,i[1]])\n",
    "    return tf_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['field', 0.043478260869565216],\n",
       "  ['information', 0.043478260869565216],\n",
       "  ['retrieval', 0.043478260869565216],\n",
       "  ['also', 0.043478260869565216],\n",
       "  ['covers', 0.043478260869565216],\n",
       "  ['supporting', 0.043478260869565216],\n",
       "  ['users', 0.043478260869565216],\n",
       "  ['browsing', 0.043478260869565216],\n",
       "  ['filtering', 0.043478260869565216],\n",
       "  ['document', 0.043478260869565216],\n",
       "  ['collections', 0.043478260869565216],\n",
       "  ['processing', 0.043478260869565216],\n",
       "  ['set', 0.08695652173913043],\n",
       "  ['retrieved', 0.043478260869565216],\n",
       "  ['documents', 0.13043478260869565],\n",
       "  ['given', 0.043478260869565216],\n",
       "  ['clustering', 0.043478260869565216],\n",
       "  ['task', 0.043478260869565216],\n",
       "  ['coming', 0.043478260869565216],\n",
       "  ['good', 0.043478260869565216],\n",
       "  ['grouping', 0.043478260869565216],\n",
       "  ['based', 0.043478260869565216],\n",
       "  ['contents', 0.043478260869565216]],\n",
       " 1]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_value(items_with_frequency)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculating idf values for all given documents\n",
    "\n",
    "def idf_value(n,distinct_item_with_count_and_docs):\n",
    "    idf_list = []\n",
    "    for i in distinct_item_with_count_and_docs:\n",
    "        if(i[1] > 0):\n",
    "            idf_val = math.log2(float(n) / i[1])\n",
    "        else:\n",
    "            idf_val = 0\n",
    "        idf_list.append([i[0],idf_val])\n",
    "    return idf_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "var4 = idf_value(14,distinct_item_with_count_and_docs)\n",
    "\n",
    "### Creating dictionary from given 2-D list\n",
    "\n",
    "idf_dict = {i[0]:i[1] for i in var4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.807354922057604"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf_dict[\"field\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculating tf-idf values for all given documents\n",
    "\n",
    "tf_idf_value = []\n",
    "for i in tf_value(items_with_frequency):\n",
    "    var7 = []\n",
    "    for j in i[0]:\n",
    "        var6 = j[1]*idf_dict[j[0]]\n",
    "        var7.append([j[0],var6])\n",
    "    tf_idf_value.append([var7,i[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tf_idf_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['field', 0.12205890965467844],\n",
       "  ['information', 0.12205890965467844],\n",
       "  ['retrieval', 0.16553717052424366],\n",
       "  ['also', 0.12205890965467844],\n",
       "  ['covers', 0.16553717052424366],\n",
       "  ['supporting', 0.16553717052424366],\n",
       "  ['users', 0.16553717052424366],\n",
       "  ['browsing', 0.16553717052424366],\n",
       "  ['filtering', 0.16553717052424366],\n",
       "  ['document', 0.12205890965467844],\n",
       "  ['collections', 0.16553717052424366],\n",
       "  ['processing', 0.16553717052424366],\n",
       "  ['set', 0.3310743410484873],\n",
       "  ['retrieved', 0.16553717052424366],\n",
       "  ['documents', 0.496611511572731],\n",
       "  ['given', 0.16553717052424366],\n",
       "  ['clustering', 0.16553717052424366],\n",
       "  ['task', 0.16553717052424366],\n",
       "  ['coming', 0.16553717052424366],\n",
       "  ['good', 0.16553717052424366],\n",
       "  ['grouping', 0.16553717052424366],\n",
       "  ['based', 0.16553717052424366],\n",
       "  ['contents', 0.16553717052424366]],\n",
       " 1]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_value[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['symptoms', 'covid-19', 'spreading']"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generating pairs of distinct words and its count from given Query\n",
    "\n",
    "unique_query = []\n",
    "for i in query:\n",
    "    if i not in unique_query:\n",
    "        unique_query.append(i)\n",
    "        \n",
    "unique_query_with_item_count = [[i,query.count(i)] for i in unique_query]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['symptoms', 1], ['covid-19', 1], ['spreading', 1]]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_query_with_item_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculating tf values for given Query\n",
    "\n",
    "def tf_value(n,unique_query_with_item_count):\n",
    "    tf_list = []\n",
    "    for i in unique_query_with_item_count:\n",
    "        query_var = i[1]/ n\n",
    "        tf_list.append([i[0],query_var])\n",
    "    return tf_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['symptoms', 0.3333333333333333],\n",
       " ['covid-19', 0.3333333333333333],\n",
       " ['spreading', 0.3333333333333333]]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_tf = tf_value(len(query),unique_query_with_item_count)\n",
    "query_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculating tf-idf values for given Query\n",
    "\n",
    "tf_idf_query = []\n",
    "for i in query_tf:\n",
    "    if i[0] not in idf_dict.keys():\n",
    "        temp3 = 0\n",
    "    else:\n",
    "        temp3 = i[1]*idf_dict[i[0]]\n",
    "    tf_idf_query.append([i[0],temp3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['symptoms', 1.2691183073525347],\n",
       " ['covid-19', 0.740797473778816],\n",
       " ['spreading', 1.2691183073525347]]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['field', 0.12205890965467844],\n",
       "  ['information', 0.12205890965467844],\n",
       "  ['retrieval', 0.16553717052424366],\n",
       "  ['also', 0.12205890965467844],\n",
       "  ['covers', 0.16553717052424366],\n",
       "  ['supporting', 0.16553717052424366],\n",
       "  ['users', 0.16553717052424366],\n",
       "  ['browsing', 0.16553717052424366],\n",
       "  ['filtering', 0.16553717052424366],\n",
       "  ['document', 0.12205890965467844],\n",
       "  ['collections', 0.16553717052424366],\n",
       "  ['processing', 0.16553717052424366],\n",
       "  ['set', 0.3310743410484873],\n",
       "  ['retrieved', 0.16553717052424366],\n",
       "  ['documents', 0.496611511572731],\n",
       "  ['given', 0.16553717052424366],\n",
       "  ['clustering', 0.16553717052424366],\n",
       "  ['task', 0.16553717052424366],\n",
       "  ['coming', 0.16553717052424366],\n",
       "  ['good', 0.16553717052424366],\n",
       "  ['grouping', 0.16553717052424366],\n",
       "  ['based', 0.16553717052424366],\n",
       "  ['contents', 0.16553717052424366]],\n",
       " 1]"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_value[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculating Cosine similarity between each documents and Given query\n",
    "\n",
    "def cosine_similarity(tf_idf_value,tf_idf_query):\n",
    "    cosine_similarity_list = []\n",
    "    query_sum_var = 0\n",
    "    for i in tf_idf_query:\n",
    "        query_sum_var = query_sum_var + (i[1]**2)\n",
    "    query_sqt_var= math.sqrt(query_sum_var)\n",
    "    \n",
    "    for i in tf_idf_value:\n",
    "        doc_sum_var = 0\n",
    "        for j in i[0]:\n",
    "            doc_sum_var = doc_sum_var + (j[1]**2)\n",
    "        doc_sqt_var = math.sqrt(doc_sum_var)\n",
    "        \n",
    "        temp4 = 0\n",
    "        for j in tf_idf_query:\n",
    "            for k in i[0]:\n",
    "                if j[0] == k[0]:\n",
    "                    temp4 = temp4 + j[1]*k[1]\n",
    "                    break\n",
    "        try:\n",
    "            temp5 = temp4/(query_sqt_var*doc_sqt_var)\n",
    "            cosine_similarity_list.append([i[1],temp5])\n",
    "            \n",
    "        except:\n",
    "            print(\"Query not found in any documents\")\n",
    "            break\n",
    "        \n",
    "        \n",
    "    return(cosine_similarity_list)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity_list = cosine_similarity(tf_idf_value,tf_idf_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cosine_similarity_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 0.0],\n",
       " [10, 0.0],\n",
       " [11, 0.0],\n",
       " [12, 0.04424466790774531],\n",
       " [13, 0.2729075411149104],\n",
       " [14, 0.17149969559674133],\n",
       " [2, 0.0],\n",
       " [3, 0.0],\n",
       " [4, 0.0],\n",
       " [5, 0.0],\n",
       " [6, 0.0],\n",
       " [7, 0.0],\n",
       " [8, 0.0],\n",
       " [9, 0.0]]"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Sorting Cosine similarity list in decending order\n",
    "\n",
    "def cosine_key(cosine_similarity_list):\n",
    "    return(cosine_similarity_list[1])\n",
    "\n",
    "cosine_similarity_list.sort(key = cosine_key,reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[13, 0.2729075411149104],\n",
       " [14, 0.17149969559674133],\n",
       " [12, 0.04424466790774531],\n",
       " [1, 0.0],\n",
       " [10, 0.0],\n",
       " [11, 0.0],\n",
       " [2, 0.0],\n",
       " [3, 0.0],\n",
       " [4, 0.0],\n",
       " [5, 0.0],\n",
       " [6, 0.0],\n",
       " [7, 0.0],\n",
       " [8, 0.0],\n",
       " [9, 0.0]]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 documents as per given Query are: \n",
      "\n",
      "Document\tCosine Similarity\n",
      "\n",
      "13 \t==>\t 0.2729075411149104\n",
      "14 \t==>\t 0.17149969559674133\n",
      "12 \t==>\t 0.04424466790774531\n"
     ]
    }
   ],
   "source": [
    "print(\"Top 3 documents as per given Query are: \")\n",
    "print()\n",
    "print(\"Document\\tCosine Similarity\")\n",
    "print()\n",
    "for i in cosine_similarity_list[:3]:\n",
    "    print(i[0],\"\\t==>\\t\",i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All documents as per given Query are: \n",
      "\n",
      "Document\tCosine Similarity\n",
      "\n",
      "13 \t==>\t 0.2729075411149104\n",
      "14 \t==>\t 0.17149969559674133\n",
      "12 \t==>\t 0.04424466790774531\n",
      "1 \t==>\t 0.0\n",
      "10 \t==>\t 0.0\n",
      "11 \t==>\t 0.0\n",
      "2 \t==>\t 0.0\n",
      "3 \t==>\t 0.0\n",
      "4 \t==>\t 0.0\n",
      "5 \t==>\t 0.0\n",
      "6 \t==>\t 0.0\n",
      "7 \t==>\t 0.0\n",
      "8 \t==>\t 0.0\n",
      "9 \t==>\t 0.0\n"
     ]
    }
   ],
   "source": [
    "print(\"All documents as per given Query are: \")\n",
    "print()\n",
    "print(\"Document\\tCosine Similarity\")\n",
    "print()\n",
    "for i in cosine_similarity_list:\n",
    "    print(i[0],\"\\t==>\\t\",i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
